{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42e2c838",
   "metadata": {},
   "source": [
    "# Notebook 2-1: HMM Training\n",
    "\n",
    "In this notebook, we will cover\n",
    "1. **Loading prepared data**: We will load the prepared data from the previous notebook.\n",
    "2. **Configuring the HMM**: We will configure the HMM with the appropriate parameters.\n",
    "3. **Training the HMM (optional)**: We will train the HMM using the prepared data.\n",
    "4. **Loading a pre-trained HMM**: We will load a pre-trained HMM for inference.\n",
    "5. **Inference with the HMM**: We will perform inference using the trained HMM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2187f5",
   "metadata": {},
   "source": [
    "## 1. Loading prepared data\n",
    "Here we will load the prepared data from the previous notebook `1_prepare-data.ipynb`. Recall that the data was prepared with TDE and PCA, before a final standardisation step was applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c39d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from osl_dynamics.data import Data\n",
    "\n",
    "data = Data(\"prepared_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c65ec0f",
   "metadata": {},
   "source": [
    "## 2. Configuring the HMM\n",
    "The HMM can be configured with the `Config` and `Model` API in the `osl_dynamics.models.hmm` module. The `Config` class is used to set up the model parameters, while the `Model` class is used to create the model object itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a191ab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from osl_dynamics.models.hmm import Config, Model\n",
    "\n",
    "config = Config(\n",
    "    n_states=6,\n",
    "    n_channels=data.n_channels,\n",
    "    sequence_length=200,\n",
    "    learn_means=False,\n",
    "    learn_covariances=True,\n",
    "    batch_size=128,\n",
    "    learning_rate=0.01,\n",
    "    n_epochs=20,\n",
    ")\n",
    "model = Model(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074bb4b1",
   "metadata": {},
   "source": [
    "Once you have created the model, you can have a look at a summary of the model by calling the `Model.summary()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c35003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cea5cd",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "- What do the options in the `Config` class do? Can you find the documentation for the `Config` class and figure out yourself?\n",
    "- Why do you see `Non-trainable params: 480 (1.88KB)` in the summary? What does this mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2213a63a",
   "metadata": {},
   "source": [
    "## 3. Training the HMM (optional - might take a while, ~20 - 60s per epoch)\n",
    "\n",
    "Now we can start training the HMM. Due to the stochastic nature of the process of training the HMM and non-convexity of the loss function, we normally initialise the model with multiple initialisations. We then continue training the model with the best performing initialisation.\n",
    "\n",
    "Here we initialise the model by training it with 3 different initialisations, each of which is trained for 1 epoch. For the HMM, this is often enough of an initialisation to get reproducible results. `n_init` could be increased if you find a high run-to-run variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db163e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_history = model.random_state_time_course_initialization(data, n_init=3, n_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724da530",
   "metadata": {},
   "source": [
    "Now we have a good initialisation, we do the full model training by calling the `Model.fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc794d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1573136f",
   "metadata": {},
   "source": [
    "After training, we can save the trained model with the `Model.save()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1b1ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"results/model\", exist_ok=True)\n",
    "model.save(\"results/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752b3cd2",
   "metadata": {},
   "source": [
    "The free energy is a measure of how well the model fits the data, with \"regularisation\" term added for measuring the complexity of the model. We can get the free energy on a dataset by calling the `Model.free_energy()` method. Here we will get the free energy on the training data and save the training history to a `.pkl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e024cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "free_energy = model.free_energy(data)\n",
    "history[\"free_energy\"] = free_energy\n",
    "\n",
    "pickle.dump(init_history, open(\"results/model/init_history.pkl\", \"wb\"))\n",
    "pickle.dump(history, open(\"results/model/history.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9983c322",
   "metadata": {},
   "source": [
    "## 4. Loading a pretrained model\n",
    "We have also supplied a pretrained model. If you have not trained the model yourself following the above steps, you can download the pretrained model by checking out the `0_get_model.ipynb` notebook.\n",
    "\n",
    "`osl-dynamics` comes with a function to load pretrained models in the `osl_dynamics.models` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3a787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from osl_dynamics.models import load\n",
    "\n",
    "model = load('results/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26574598",
   "metadata": {},
   "source": [
    "## 5. Inference with the HMM\n",
    "Now that we have a trained HMM, we can use it to infer useful information about the data. These include:\n",
    "- The state time courses (posterior state probabilities - $\\alpha$)\n",
    "- The state means.\n",
    "- The state covariance matrices.\n",
    "- The probability transition matrix.\n",
    "- The initial state probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0d6617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "inf_params_dir = \"results/inf_params\"\n",
    "os.makedirs(inf_params_dir, exist_ok=True)\n",
    "\n",
    "alp = model.get_alpha(data)\n",
    "means = model.get_means()\n",
    "covs = model.get_covariances()\n",
    "trans_prob = model.get_trans_prob()\n",
    "initial_state_probs = model.get_initial_state_probs()\n",
    "\n",
    "pickle.dump(alp, open(f\"{inf_params_dir}/alp.pkl\", \"wb\"))\n",
    "np.save(f\"{inf_params_dir}/means.npy\", means)\n",
    "np.save(f\"{inf_params_dir}/covs.npy\", covs)\n",
    "np.save(f\"{inf_params_dir}/trans_prob.npy\", trans_prob)\n",
    "np.save(f\"{inf_params_dir}/initial_state_probs.npy\", initial_state_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7289686",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "How do you get the viterbi path?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad847843",
   "metadata": {},
   "outputs": [],
   "source": [
    "viterbi_path = model.get_viterbi_path(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36713ea-efff-4715-8cdf-50faad301309",
   "metadata": {},
   "source": [
    "We can plot the viterbi path of the first 8 seconds of the first session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b554c32f-c794-41d0-a708-2ba57f423765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from osl_dynamics.utils import plotting\n",
    "\n",
    "plotting.plot_alpha(\n",
    "    viterbi_path[0],\n",
    "    n_samples=2000,\n",
    "    sampling_frequency=250,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2decc2f0",
   "metadata": {},
   "source": [
    "Note: The viterbi path is the most likely sequence of hidden states given the observed data. The difference between the viterbi path and directly using the argmax of the posterior state probabilities is that the viterbi path maximises the joint posterior distribution of the entire sequence of hidden states given the observed data, while the argmax of the posterior state probabilities only maximises the marginal posterior distribution.\n",
    "\n",
    "Normally we would use the argmax of the posterior state probabilities due to the fact that the viterbi path can be computationally expensive to calculate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
